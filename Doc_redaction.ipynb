{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import docx\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "def create_test_docx():\n",
        "    doc = docx.Document()\n",
        "    doc.add_paragraph(\"Employee Report for Microsoft Corporation\")\n",
        "    doc.add_paragraph(\"Contact: John Smith (email: john.smith@microsoft.com)\")\n",
        "    doc.add_paragraph(\"Location: 123 Main Street, Seattle, Washington 98104\")\n",
        "    doc.add_paragraph(\"Project budget: $500,000\")\n",
        "    doc.add_paragraph(\"Team members visited London and Tokyo last quarter.\")\n",
        "    doc.save(\"document.docx\")\n",
        "\n",
        "def create_test_xml():\n",
        "    root = ET.Element(\"company\")\n",
        "    employee = ET.SubElement(root, \"employee\")\n",
        "    ET.SubElement(employee, \"name\").text = \"Sarah Johnson\"\n",
        "    ET.SubElement(employee, \"phone\").text = \"+1 (555) 123-4567\"\n",
        "    ET.SubElement(employee, \"address\").text = \"456 Oak Avenue, Chicago, Illinois 60601\"\n",
        "    tree = ET.ElementTree(root)\n",
        "    tree.write(\"data.xml\")\n",
        "\n",
        "def create_test_json():\n",
        "    data = {\n",
        "        \"organization\": \"Apple Inc.\",\n",
        "        \"employees\": [\n",
        "            {\n",
        "                \"name\": \"David Brown\",\n",
        "                \"email\": \"david.brown@apple.com\",\n",
        "                \"location\": \"Cupertino, California\"\n",
        "            }\n",
        "        ],\n",
        "        \"transactions\": [\n",
        "            {\n",
        "                \"amount\": \"$750,000\",\n",
        "                \"date\": \"2024-01-15\"\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "    with open(\"data.json\", \"w\") as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    create_test_docx()\n",
        "    create_test_xml()\n",
        "    create_test_json()\n",
        "    print(\"Test documents created successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HfLUHSpR_6g",
        "outputId": "2c22eb9e-62b8-47b1-f607-b5e92a98c303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test documents created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import spacy\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image, ImageDraw\n",
        "from typing import List, Set, Tuple, Dict\n",
        "import docx\n",
        "from xml.etree import ElementTree as ET\n",
        "import json\n",
        "\n",
        "class RedactionError(Exception):\n",
        "    \"\"\"Custom exception for redaction errors.\"\"\"\n",
        "    pass\n",
        "\n",
        "def get_file_info():\n",
        "    \"\"\"Get file type and path from user input.\"\"\"\n",
        "    print(\"\\nSupported file types: docx, xml, json, pdf\")\n",
        "    file_type = input(\"Enter file type to process: \").lower().strip()\n",
        "    file_path = input(\"Enter file path: \").strip()\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
        "    if file_type not in ['docx', 'xml', 'json', 'pdf']:\n",
        "        raise ValueError(f\"Unsupported file type: {file_type}\")\n",
        "\n",
        "    return file_type, file_path\n",
        "\n",
        "def get_pattern_types() -> List[str]:\n",
        "    \"\"\"Get pattern types to redact from user input.\"\"\"\n",
        "    available_patterns = ['EMAIL', 'PHONE', 'SSN', 'CREDIT_CARD', 'AADHAAR', 'PAN']\n",
        "    print(\"\\nAvailable pattern types:\")\n",
        "    print(\", \".join(available_patterns))\n",
        "\n",
        "    selected_patterns = input(\"\\nEnter pattern types to redact (comma-separated) or press Enter to skip: \").strip()\n",
        "    if not selected_patterns:\n",
        "        return []\n",
        "    return [p.strip().upper() for p in selected_patterns.split(',')]\n",
        "\n",
        "def get_entities_to_redact() -> Tuple[List[str], Set[str], List[str]]:\n",
        "    \"\"\"Get entity types, custom terms, and pattern types to redact from user input.\"\"\"\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    entity_types = nlp.pipe_labels['ner']\n",
        "\n",
        "    print(\"\\nAvailable entity types:\")\n",
        "    print(\", \".join(entity_types))\n",
        "\n",
        "    selected_types = input(\"\\nEnter entity types to redact (comma-separated) or 'all': \").strip()\n",
        "    pattern_types = get_pattern_types()\n",
        "    custom_terms = input(\"Enter additional terms to redact (comma-separated) or press Enter to skip: \").strip()\n",
        "\n",
        "    ent_types = [t.strip().upper() for t in selected_types.split(',')] if selected_types != 'all' else entity_types\n",
        "    terms = set(t.strip() for t in custom_terms.split(',')) if custom_terms else set()\n",
        "\n",
        "    return ent_types, terms, pattern_types\n",
        "\n",
        "def find_pattern_matches(text: str, pattern_types: List[str]) -> Set[str]:\n",
        "    \"\"\"Find sensitive terms using selected regex patterns.\"\"\"\n",
        "    patterns = {\n",
        "        'EMAIL': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
        "        'PHONE': r'\\+?1?\\s*\\(?[0-9]{3}\\)?[-.\\s]?[0-9]{3}[-.\\s]?[0-9]{4}',\n",
        "        'SSN': r'\\b\\d{3}[-.]?\\d{2}[-.]?\\d{4}\\b',\n",
        "        'CREDIT_CARD': r'\\b\\d{4}[-. ]?\\d{4}[-. ]?\\d{4}[-. ]?\\d{4}\\b',\n",
        "        'AADHAAR': r'\\b[2-9]{1}[0-9]{3}[-. ]?[0-9]{4}[-. ]?[0-9]{4}\\b',\n",
        "        'PAN': r'\\b[A-Z]{5}[0-9]{4}[A-Z]{1}\\b'\n",
        "    }\n",
        "\n",
        "    sensitive_terms = set()\n",
        "    for pattern_type in pattern_types:\n",
        "        if pattern_type in patterns:\n",
        "            matches = re.findall(patterns[pattern_type], text)\n",
        "            sensitive_terms.update(matches)\n",
        "\n",
        "    return sensitive_terms\n",
        "\n",
        "def identify_entities(text: str, ent_types: List[str], pattern_types: List[str]) -> Set[str]:\n",
        "    \"\"\"Identify entities to redact using spaCy NER and selected pattern matching.\"\"\"\n",
        "    terms = set()\n",
        "\n",
        "    # SpaCy NER\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(text)\n",
        "    terms.update({ent.text for ent in doc.ents if ent.label_ in ent_types})\n",
        "\n",
        "    # Pattern matching for selected patterns\n",
        "    terms.update(find_pattern_matches(text, pattern_types))\n",
        "\n",
        "    return terms\n",
        "\n",
        "def redact_text(text: str, terms: Set[str]) -> str:\n",
        "    \"\"\"Redact terms in text by replacing with asterisks.\"\"\"\n",
        "    for term in sorted(terms, key=len, reverse=True):\n",
        "        if term and not term.isspace():\n",
        "            text = text.replace(term, '*' * len(term))\n",
        "    return text\n",
        "\n",
        "def redact_docx(doc_path: str, ent_types: List[str], custom_terms: Set[str], pattern_types: List[str]):\n",
        "    \"\"\"Redact sensitive information in a DOCX file.\"\"\"\n",
        "    doc = docx.Document(doc_path)\n",
        "    terms = custom_terms.copy()\n",
        "\n",
        "    for paragraph in doc.paragraphs:\n",
        "        text = paragraph.text\n",
        "        terms.update(identify_entities(text, ent_types, pattern_types))\n",
        "        for run in paragraph.runs:\n",
        "            run.text = redact_text(run.text, terms)\n",
        "\n",
        "    for table in doc.tables:\n",
        "        for row in table.rows:\n",
        "            for cell in row.cells:\n",
        "                for paragraph in cell.paragraphs:\n",
        "                    text = paragraph.text\n",
        "                    terms.update(identify_entities(text, ent_types, pattern_types))\n",
        "                    for run in paragraph.runs:\n",
        "                        run.text = redact_text(run.text, terms)\n",
        "\n",
        "    new_path = get_redacted_path(doc_path)\n",
        "    doc.save(new_path)\n",
        "    return terms\n",
        "\n",
        "def redact_xml(xml_path: str, ent_types: List[str], custom_terms: Set[str], pattern_types: List[str]):\n",
        "    \"\"\"Redact sensitive information in an XML file.\"\"\"\n",
        "    tree = ET.parse(xml_path)\n",
        "    root = tree.getroot()\n",
        "    terms = custom_terms.copy()\n",
        "\n",
        "    for element in root.iter():\n",
        "        if element.text:\n",
        "            terms.update(identify_entities(element.text, ent_types, pattern_types))\n",
        "            element.text = redact_text(element.text, terms)\n",
        "\n",
        "    new_path = get_redacted_path(xml_path)\n",
        "    tree.write(new_path)\n",
        "    return terms\n",
        "\n",
        "def redact_json(json_path: str, ent_types: List[str], custom_terms: Set[str], pattern_types: List[str]):\n",
        "    \"\"\"Redact sensitive information in a JSON file.\"\"\"\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    terms = custom_terms.copy()\n",
        "\n",
        "    def redact_value(value):\n",
        "        if isinstance(value, str):\n",
        "            terms.update(identify_entities(value, ent_types, pattern_types))\n",
        "            return redact_text(value, terms)\n",
        "        elif isinstance(value, list):\n",
        "            return [redact_value(v) for v in value]\n",
        "        elif isinstance(value, dict):\n",
        "            return {k: redact_value(v) for k, v in value.items()}\n",
        "        return value\n",
        "\n",
        "    redacted_data = redact_value(data)\n",
        "    new_path = get_redacted_path(json_path)\n",
        "\n",
        "    with open(new_path, 'w') as f:\n",
        "        json.dump(redacted_data, f, indent=4)\n",
        "\n",
        "    return terms\n",
        "\n",
        "def redact_image(image: Image.Image, ocr_data: dict, terms_to_redact: Set[str]) -> Image.Image:\n",
        "    \"\"\"Redact sensitive text in the image using word-level bounding boxes.\"\"\"\n",
        "    try:\n",
        "        redacted_image = image.copy()\n",
        "        draw = ImageDraw.Draw(redacted_image)\n",
        "\n",
        "        n_boxes = len(ocr_data['text'])\n",
        "        for i in range(n_boxes):\n",
        "            if int(ocr_data['conf'][i]) > 0:\n",
        "                word = ocr_data['text'][i]\n",
        "\n",
        "                # Check if word should be redacted\n",
        "                should_redact = False\n",
        "                for term in terms_to_redact:\n",
        "                    if term and not term.isspace():\n",
        "                        if term.lower() in word.lower() or word.lower() in term.lower():\n",
        "                            should_redact = True\n",
        "                            break\n",
        "\n",
        "                if should_redact:\n",
        "                    x = ocr_data['left'][i]\n",
        "                    y = ocr_data['top'][i]\n",
        "                    w = ocr_data['width'][i]\n",
        "                    h = ocr_data['height'][i]\n",
        "\n",
        "                    # Add padding\n",
        "                    padding = 2\n",
        "                    draw.rectangle([\n",
        "                        (x - padding, y - padding),\n",
        "                        (x + w + padding, y + h + padding)\n",
        "                    ], fill=\"black\")\n",
        "\n",
        "        return redacted_image\n",
        "    except Exception as e:\n",
        "        raise RedactionError(f\"Image redaction failed: {str(e)}\")\n",
        "\n",
        "def convert_images_to_pdf(images: List[Image.Image], output_path: str) -> None:\n",
        "    \"\"\"Convert a list of images to a PDF file.\"\"\"\n",
        "    try:\n",
        "        rgb_images = [img.convert('RGB') for img in images]\n",
        "        rgb_images[0].save(\n",
        "            output_path,\n",
        "            save_all=True,\n",
        "            append_images=rgb_images[1:],\n",
        "            resolution=100.0\n",
        "        )\n",
        "    except Exception as e:\n",
        "        raise RedactionError(f\"Failed to create PDF: {str(e)}\")\n",
        "\n",
        "def process_pdf(pdf_path: str, ent_types: List[str], custom_terms: Set[str], pattern_types: List[str]):\n",
        "    \"\"\"Process PDF using OCR and image-based redaction.\"\"\"\n",
        "    try:\n",
        "        print(\"Converting PDF to images...\")\n",
        "        images = convert_from_path(pdf_path)\n",
        "\n",
        "        redacted_images = []\n",
        "        terms = custom_terms.copy()\n",
        "\n",
        "        for i, image in enumerate(images, 1):\n",
        "            print(f\"Processing page {i} of {len(images)}...\")\n",
        "\n",
        "            text = pytesseract.image_to_string(image)\n",
        "            ocr_data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)\n",
        "\n",
        "            page_terms = identify_entities(text, ent_types, pattern_types)\n",
        "            terms.update(page_terms)\n",
        "\n",
        "            if page_terms:\n",
        "                print(f\"Found {len(page_terms)} items to redact on page {i}\")\n",
        "\n",
        "            redacted_image = redact_image(image, ocr_data, terms)\n",
        "            redacted_images.append(redacted_image)\n",
        "\n",
        "        new_path = get_redacted_path(pdf_path)\n",
        "        print(\"Creating redacted PDF...\")\n",
        "        convert_images_to_pdf(redacted_images, new_path)\n",
        "        print(f\"Redacted PDF saved as: {new_path}\")\n",
        "\n",
        "        return terms\n",
        "\n",
        "    except Exception as e:\n",
        "        raise RedactionError(f\"PDF processing failed: {str(e)}\")\n",
        "\n",
        "def get_redacted_path(file_path: str) -> str:\n",
        "    \"\"\"Generate path for redacted file.\"\"\"\n",
        "    base, ext = os.path.splitext(file_path)\n",
        "    return f\"{base}_redacted{ext}\"\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # Get file information and redaction settings\n",
        "        file_type, file_path = get_file_info()\n",
        "        ent_types, custom_terms, pattern_types = get_entities_to_redact()\n",
        "\n",
        "        # Process based on file type\n",
        "        redaction_funcs = {\n",
        "            'docx': redact_docx,\n",
        "            'xml': redact_xml,\n",
        "            'json': redact_json,\n",
        "            'pdf': process_pdf\n",
        "        }\n",
        "\n",
        "        terms = redaction_funcs[file_type](file_path, ent_types, custom_terms, pattern_types)\n",
        "\n",
        "        print(f\"\\nProcessed {file_path}\")\n",
        "        print(\"Redacted terms:\", ', '.join(sorted(terms)))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError: {str(e)}\")\n",
        "        return 1\n",
        "\n",
        "    return 0\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    exit(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgvxK7Gvs1HA",
        "outputId": "19510172-7b17-4b99-bbb3-d49440ec3e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Supported file types: docx, xml, json, pdf\n",
            "Enter file type to process: xml\n",
            "Enter file path: data.xml\n",
            "\n",
            "Available entity types:\n",
            "CARDINAL, DATE, EVENT, FAC, GPE, LANGUAGE, LAW, LOC, MONEY, NORP, ORDINAL, ORG, PERCENT, PERSON, PRODUCT, QUANTITY, TIME, WORK_OF_ART\n",
            "\n",
            "Enter entity types to redact (comma-separated) or 'all': person,gpe\n",
            "\n",
            "Available pattern types:\n",
            "EMAIL, PHONE, SSN, CREDIT_CARD, AADHAAR, PAN\n",
            "\n",
            "Enter pattern types to redact (comma-separated) or press Enter to skip: email\n",
            "Enter additional terms to redact (comma-separated) or press Enter to skip: \n",
            "\n",
            "Processed data.xml\n",
            "Redacted terms: Chicago, Illinois, Oak Avenue, Sarah Johnson\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KgJO9Oxouc5e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}